Title,Abstract,OpenReview Link,Authors,Status
Decoder-as-Policy: Head-Only PPO Fine-Tuning of a Spike-Transformer for Low-Error Kinematic Decoding,"Spike-token transformers such as POYO achieve strong across-session decoding, yet purely supervised training can overweight variance alignment (explained variance) relative to the pointwise accuracy needed for closed-loop BCI control. We treat the decoder’s velocity head as a Gaussian policy and fine-tune it head-only: a behavior-cloning (BC) warm start followed by on-policy PPO on a control-aligned reward (negative MSE plus a small entropy bonus and an optional variance-calibration term), while keeping the POYO encoder frozen. On \textit{NLB'21 mc\_maze\_medium}, extended BC (1--2k steps) followed by PPO reveals a broad Pareto window with very low error and high explained variance, best $R^2=0.9975$ and MSE$=0.0023$ at the same validation checkpoint (step~1900), with predictive scale ($\sigma\approx0.993$). On a separate Perich\_Miller dataset trained with 400 step, the POYO+ achieved $R^2\approx0.87$ (MSE$\approx0.34$) after PPO fine tuning. We provide leakage safeguards, ablations, and reproducible configs.",https://openreview.net/forum?id=Yuif16WbgR,"Fengge Liang, Cong Wang, Shiqian Shen",Poster
PrimateFace: A Foundational Resource for Generalizable Cross-Species Facial Analysis,"The analysis of facial kinematics, a critical class of behavioral biosignals, is fundamental to understanding social cognition. Progress has been hampered by the lack of large-scale, diverse datasets needed to train robust and generalizable models for face detection and facial landmark estimation (FLE). We introduce PrimateFace, a foundational resource designed to overcome this bottleneck. It consists of a large-scale dataset of over 260,000 images spanning more than 60 primate genera, and a suite of pretrained models. Models trained on PrimateFace achieve high cross-species performance, from tarsiers to gorillas, and demonstrate remarkable generalization to human data, validating the benefits of pre-training on taxonomically diverse data. We showcase how PrimateFace serves as an essential front-end for diverse downstream applications, including quantifying social gaze in human infants, enabling multimodal analysis of vocalizations, and powering the data-driven discovery of behavioral repertoires. PrimateFace provides a standardized platform for extracting and analyzing behavioral biosignals, empowering scalable, data-driven studies of behavior.",https://openreview.net/forum?id=zk5cYUJQ0C,"Felipe Parodi, Konrad Kording",Poster
"Towards a generalizable, unified framework for decoding from multimodal neural activity","Recent advances in neural decoding have led to the development of large-scale deep learning-based neural decoders that can generalize across sessions and subjects. However, existing approaches predominantly focus on single modalities of neural activity, limiting their applicability to specific modalities and tasks. In this work, we present a multimodal extension of the POYO framework that jointly processes neuronal spikes and local field potentials (LFPs) for behavioural decoding. Our approach employs flexible tokenization schemes for both spikes and LFPs, enabling efficient processing of heterogeneous neural populations without preprocessing requirements like binning. Through experiments on data from nonhuman primates performing motor tasks, we demonstrate that multimodal pretraining yields superior decoding performance compared to unimodal baselines. We also show evidence of cross-modal transfer: models pretrained on both modalities outperform LFP-only models when fine-tuned solely on LFPs, suggesting a path toward more cost-effective brain-computer interfaces that can use performant LFP-based decoders. Our models also exhibit robustness to missing modalities during inference when trained with modality masking, and scale effectively with both model size and pretraining data. Overall, this work represents an important first step towards unified, general-purpose neural decoders capable of leveraging diverse neural signals for a variety of brain-computer interface applications.",https://openreview.net/forum?id=ZMRxom1HKo,"Nanda H Krishna, Mathys Loiselle, Avery Hee-Woon Ryoo, Matthew G Perich, Guillaume Lajoie",Poster
Evaluating Foundation Models for the Brain: A Dynamical Systems Perspective,"Foundation models promise to transform neuroscience and brain–computer interfaces (BCIs), but their evaluation remains fragmented and often misleading. Standard benchmarks that emphasize in-distribution accuracy fail to capture what truly matters in dynamical domains: the ability to generalize across conditions that differ from those seen during training. In this perspective, we propose a unified framework for evaluating brain foundation models through the lens of dynamical systems theory. We introduce a generalization spectrum—a hierarchy of distribution shifts spanning system identity, parameter regimes, attractor structure, initial conditions, and observation noise—that clarifies what kinds of robustness should be expected from models claiming to be ``foundational.'' We then map this spectrum onto a brain-specific taxonomy of distribution shifts—surface (hardware and noise), functional (state and task), and structural (subject and species)—to ground the framework in neuroscientific practice. Building on this foundation, we outline success criteria for brain foundation models: strong out-of-distribution generalization with minimal data, benchmark-validated performance across diverse tasks, and scalable power-law improvements with model and dataset size. This framework provides a common language for machine learning, neuroscience, and clinical research, and offers a roadmap for building evaluation cultures that distinguish narrow task solvers from truly foundational models.",https://openreview.net/forum?id=CnT60CEfPa,Max Kanwal,Poster
Unified Pretraining on Mixed Optophysiology and Electrophysiology Data Across Brain Regions,"Building models that unify diverse neural recordings is a crucial step toward scalable foundation models for neuroscience. However, most large-scale models remain tied to a single modality, which limits our ability to integrate information across different spatiotemporal scales. We introduce a POYO-based universal encoder that learns a shared latent representation of electrophysiology (irregular spike times) and optophysiology (regular timeseries) without requiring simultaneous recordings. Across large datasets from the Allen Institute spanning both calcium imaging and Neuropixels, we show that joint pretraining outperforms uni-modal baselines and strengthens cross-region transfer. These results show that our mixed-modality pretraining framework can integrate independently collected recordings into a common representational space, advancing the path toward foundation models for diverse multi-modal neural data.",https://openreview.net/forum?id=bM3ZR0MK0f,"Ian Jarratt Knight, Vinam Arora, Mehdi Azabou, Eva L Dyer",Spotlight
NeuroMamba: A State-Space Foundation Model for Functional MRI,"Foundation models for whole-brain fMRI analysis are dominated by two competing paradigms: Region-of-Interest (ROI) approaches that discard fine-grained spatial information, and hierarchical models with Transformer or Mamba backbones that retain it. However, the rigid, grid-based architecture of hierarchical models imposes critical limitations. They are forced to process vast, uninformative non-brain background regions (up to 60\% of the data), creating significant computational inefficiency, and struggle with inter-subject anatomical variation. To overcome these challenges, we introduce NeuroMamba, a foundation model that enables direct sequence modeling of 4D whole-brain fMRI. NeuroMamba leverages Mamba backbone trained with an autoregressive objective given small spatiotemporal patches. Our approach facilitates the adaptive removal of nuisance background signals—a core inefficiency bottleneck for prior grid-based methods. To address anatomical variability, the model incorporates explicit frequency-based positional encodings, enhancing robustness across subjects. While direct sequence modeling is computationally demanding, the linear-time efficiency of Mamba, coupled with adaptive background suppression, makes whole-brain pre-training tractable. We demonstrate the first successful autoregressive pre-training on full-resolution fMRI, achieving state-of-the-art accuracy for sex classification and establishing a scalable paradigm for neuroimaging analysis.",https://openreview.net/forum?id=kftg4lmQi8,"Jubin Choi, David Keetae Park, Junbeom Kwon, Shinjae Yoo, Jiook Cha",Spotlight
A scalable self-supervised method for modeling human intracranial recordings during natural behavior,"Understanding how the brain supports natural behavior is an increasingly central goal in human neuroscience. Recordings from human neurosurgical patients with intracranial EEG electrodes offer direct access to widespread brain electrical activity during a variety of behaviors over extended times. Despite the progress in the field, utilizing these recordings at scale to identify the neural underpinnings of natural human behavior remains difficult due to variability in electrode placement, channel geometry, and behavioral diversity across participants and sessions. To address these challenges, we introduce a self-supervised framework for multi-participant intracranial neural data. We use a Perceiver-based architecture to reconstruct masked channels of neural activity from unmasked channels using learnable embeddings of the channel identity and contextual information, capturing inter-channel dependencies without requiring labels. Finetuning of our self-supervised model has improved the decoding performance on a panel of downstream tasks, highlighting the potential of self-supervised learning to enable general-purpose neural decoding and support scalable integration of naturalistic human brain recordings.",https://openreview.net/forum?id=CdP8Y4K4fz,"Shivashriganesh P. Mahato, Jingyun Xiao, Alexandre Andre, Geeling Chau, Wenrui Ma, Ian Jarratt Knight, Duy Nguyen, Lawrence Jianqiao Hu, Bingni W Brunton, Michael S. Beauchamp, Bijan Pesaran, Sergey A. Shuvaev, Eva L Dyer",Spotlight
Resource-Efficient ECG Foundation Networks via Layer-wise Adaptive Compression,"Foundation models for biosignals, such as wearable ECG monitors, face challenges in resource-constrained settings due to high memory and computational demands. In this work, we propose an adaptive layer-wise compression framework that leverages quantization and pruning to reduce model size while preserving predictive performance. Layer importance, estimated via parameter contribution and weight variance, guides fine-grained assignment of bit-widths and pruning thresholds, balancing efficiency and accuracy across high- and low-sensitivity layers. Extensive experiments on Chapman and CPSC ECG datasets demonstrate that our method consistently outperforms fixed global model compression schemes, achieving up to 10.44$\times$ compression without any loss. Our architecture-agnostic framework scales to both lightweight residual networks and large foundation models, enabling real-time, low-resource ECG monitoring and advancing scalable biosignal AI for edge and mobile health applications.",https://openreview.net/forum?id=ByIyc4x8GD,"Sudhanshu Gaurhar, Tushar Shinde, Anil Kumar Tiwari",Poster
Learning Structured Sleep Transitions with Sequential EEG Foundation Models,"In this paper, we investigate a novel EEG foundation model designed to capture long-term temporal dependencies in EEG sequences and reduce implausible predictions in sleep stage annotation. Unlike existing models that treat short EEG epochs as independent samples, our method aggregates a sequence of pre-tokenized EEG epochs and learns structured dynamics spanning multiple stages. We adopt a masked language modeling framework, leveraging masked token prediction to enable robust temporal representation learning. Empirical results on the SHHS dataset show that our model outperforms four state-of-the-art EEG foundation models across standard classification metrics. Moreover, we introduce a novel metric—Irregular Transition Rate to assess the biological plausibility of stage transitions. Our method significantly reduces ITR to 15.2\%, compared to 29.6\% (BIOT) and 33.7\% (EEGPT), confirming its superior ability to model coherent sleep dynamics.",https://openreview.net/forum?id=uqHvhm6BQM,"Koki Yoshida, Zheng Chen, Rikuto Kotoge, Yasuko Matsubara, Yasushi Sakurai",Poster
A VQ-VAE framework for modeling physiological information in fMRI,"The coupling between functional magnetic resonance imaging (fMRI) and autonomic physiological processes is not merely “noise”, but captures important brain-body interactions. This relationship is likely bidirectional: autonomic fluctuations can influence brain activity and hemodynamics, and neuronal activity can modulate systemic physiological processes. This complexity makes it difficult for traditional linear approaches to fully characterize their relationship. Here, we introduce a novel application of vector quantized variational autoencoder (VQ-VAE) to characterize whole-brain patterns associated with respiration using the discretized fMRI latent space. Further, we demonstrate that this framework can be leveraged to assess the quality of fMRI-based reconstructions of low-frequency respiratory fluctuations when physiological recordings are missing or corrupted.",https://openreview.net/forum?id=wf4wqNSngi,"Shiyu Wang, Yamin Li, Ziyuan Xu, Haatef Pourmotabbed, Chang Li, Roza G Bayrak, Catie Chang",Poster
Handwriting decoding as a challenging Motor Imagery task for EEG Foundation Models,"Foundation Models (FMs) for EEG have achieved state-of-the-art performance on multiple Motor Imagery (MI) datasets, indicating their potential to provide robust, generalizable representations across diverse contexts. In this work, we investigate handwriting decoding as a challenging MI task to evaluate the generalizability of FMs for EEG. Recent studies in handwriting decoding have reported higher-than-chance performance in decoding handwritten letters from EEG. However, all prior works have attempted to decode handwriting from EEG during actual motion. Furthermore, they assume that precise movement-onset is known. We introduce a setting closer to real-world use where either movement-onset is not known or movement does not occur at all, fully utilizing motor imagery. Crucially, we find that current FMs for EEG, despite showing SOTA performance in multiple MI datasets (hand vs foot classification) are outperformed by a task-specific EEGNet model in this fine-grained task. In parallel, we also investigate avenues that are most promising for improving decoding performance. In our 4-letter classification task, we show that (a) Knowledge of movement-onset is crucial to reported decoding performance in prior works, with average performance across subjects dropping from $41.4\%$ to $32.4\%$. (b) Increasing test-time signal quality provides significant performance improvements ($45\%$ to $78\%$ in our best subject) compared to scaling training data with single-trial EEG. (c) Fully imagined handwriting can be decoded from EEG with higher-than-chance performance.",https://openreview.net/forum?id=INzFFP3Meu,"Srinivas Ravishankar, Virginia R. de Sa",Poster
Human Sensory-Musculoskeletal Modeling and Control of Whole-Body Movements,"Coordinated human movement depends on the integration of multisensory inputs, sensorimotor transformation, and motor execution, as well as sensory feedback resulting from body-environment interaction. Building dynamic models of the sensory-musculoskeletal system is essential for understanding movement control and investigating human behaviors. Here, we report a human sensory-musculoskeletal model, termed SMS-Human, that integrates precise anatomical representations of bones, joints, and muscle-tendon units with multimodal sensory inputs involving visual, vestibular, proprioceptive, and tactile components. A stage-wise hierarchical deep reinforcement learning framework was developed to address the inherent challenges of high-dimensional control in musculoskeletal systems with integrated multisensory information. Using this framework, we demonstrated the simulation of three representative movement tasks, including bipedal locomotion, vision-guided object manipulation, and human-machine interaction during bicycling. Our results showed a close resemblance between natural and simulated human motor behaviors. The simulation also revealed musculoskeletal dynamics that could not be directly measured. This work sheds deeper insights into the sensorimotor dynamics of human movements, facilitates quantitative understanding of human behaviors in interactive contexts, and informs the design of systems with embodied intelligence.",https://openreview.net/forum?id=ZIW6DwDgbm,"Chenhui Zuo, Guohao lin, Chen Zhang, Shanning Zhuang, Yanan Sui",Spotlight
Scaling Vision Transformers for Functional MRI with Flat Maps,"This paper proposes cortical flat maps as a data representation for training functional MRI (fMRI) foundation models. fMRI data in native 4D volume space are first projected onto the cortical surface, then flattened and resampled as sequences of 2D images. We train Vision Transformers on 2.3K hours of fMRI flat map ``videos'' from the Human Connectome Project using the spatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI modeling performance improves with dataset size according to a strict power scaling law. Downstream classification benchmarks show that our model learns rich representations supporting both fine-grained state decoding across subjects, as well as subject-specific trait decoding across changes in brain state. Our code and datasets are available at [links withheld].",https://openreview.net/forum?id=L0CpmKEVHw,"Connor Lane, Daniel Z Kaplan, Tanishq Mathew Abraham, Paul Steven Scotti",Poster
The Neural Pile: 476 billion tokens of broad-coverage spiking neural activity data,"Foundation models pretrained with large-scale and rich domain-specific datasets facilitate scientific discovery and technological advances. Systems neuroscience currently lacks such foundation models, due mainly to two obstacles: (i) a lack of large-scale datasets and (ii) scarcity of large-scale compute to train high-capacity models. Here, we aim to address both challenges. We first introduce the Neural Pile, a large-scale curated dataset of spiking neural activity recorded from both primates and rodents. The dataset contains 34B uncompressed tokens of neural data from primates and 441B uncompressed tokens of neural data from rodents, involving multiple species and covering a wide range of brain regions, behaviors, and tasks. We provide a separate test split that is intended as a challenging neural prediction benchmark for evaluating neural foundation models. Secondly, as a strong baseline on this benchmark, we also release large-scale models (8B parameter models with a context length of 131k tokens) pretrained on the Neural Pile.",https://openreview.net/forum?id=NKFxs7UvN9,"Emin Orhan, Feiyi Wang",Poster
Scalable Diffusion Transformer for Conditional 4D fMRI Synthesis,"Generating whole-brain 4D fMRI sequences conditioned on cognitive tasks remains challenging due to the high-dimensional, heterogeneous BOLD dynamics across subjects/acquisitions and the lack of neuroscience-grounded validation. We introduce the first diffusion transformer for voxelwise 4D fMRI conditional generation, combining 3D VQ-GAN latent compression with a CNN–Transformer backbone and strong task conditioning via AdaLN-Zero and cross-attention. On HCP task fMRI, our model reproduces task-evoked activation maps, preserves the inter-task representational structure observed in real data (RSA), achieves perfect condition specificity, and aligns ROI time-courses with canonical hemodynamic responses. Performance improves predictably with scale, reaching task-evoked map correlation of 0.83 and RSA of 0.98, consistently surpassing a U-Net baseline on all metrics. By coupling latent diffusion with a scalable backbone and strong conditioning, this work establishes a practical path to conditional 4D fMRI synthesis, paving the way for future applications such as virtual experiments, cross-site harmonization, and principled augmentation for downstream neuroimaging models.",https://openreview.net/forum?id=DsghCw8ybh,"Jungwoo Seo, David Keetae Park, Shinjae Yoo, Jiook Cha",Poster
Exploring RAG-driven Multimodal LLMs for Explainable ECG Interpretation,"Foundation models offer a path toward safer, more generalizable biosignal analysis, yet their role in clinical Electrocardiograms (ECGs) interpretation remains unclear. We present a multimodal framework with Retrieval-Augmented Generation (RAG) that couples 12-lead ECG waveforms with structured patient metadata (age, sex) to guide large language models (LLMs) and a downstream ECG encoder. Using PTB-XL for benchmarking and MIMIC-IV-ECG for pretraining, we compare four LLMs within the same RAG pipeline—GPT-4, GPT-3.5, Claude 3.5 Sonnet, and Llama 3.2—for arrhythmia classification across five diagnostic superclasses. GPT-4 achieves the best overall performance (AUC = 0.940) and maintains stable accuracy when demographic features are ablated or added, suggesting reduced sensitivity to demographic shifts. Metadata provided the largest relative gains for models with lower performance in our benchmark, while qualitative analysis highlighted remaining failure modes such as hallucinations and retrieval mismatches. These results indicate that RAG-driven multimodal prompting can enhance ECG interpretation and fairness when paired with high-performing foundation models, and we outline practical levers—retrieval design, prompt structure, and metadata integration—for building equitable and scalable clinical ECG systems.",https://openreview.net/forum?id=bCG7sMA5Bt,"Minsol Kim, Du Liu, Ragasudha Botta, Man Jiang, Jason Gusdorf, Seok Gyu Han, Patricia Maes, Paul Pu Liang",Poster
A Sensing Whole Brain Zebrafish Foundation Model for Neuron Dynamics and Behavior,"Neural dynamics underlie behaviors from memory to sleep, yet identifying mechanisms for higher-order phenomena (e.g., social interaction) is experimentally challenging. Existing whole-brain models often fail to scale to single-neuron resolution, omit behavioral readouts, or rely on PCA/conv pipelines that miss long-range, non-linear interactions. We introduce a sparse-attention whole-brain foundation model (SBM) for larval zebrafish that forecasts neuron spike probabilities conditioned on sensory stimuli and links brain state to behavior. SBM factorizes attention across neurons and along time, enabling whole-brain scale and interpretability. On a held-out subject, it achieves mean absolute error <0.02 with calibrated predictions and stable autoregressive rollouts. Coupled to a permutation-invariant behavior head, SBM enables gradient-based synthesis of neural patterns that elicit target behaviors. This framework supports rapid, behavior-grounded exploration of complex neural phenomena.",https://openreview.net/forum?id=wDSXEDsXnl,"Sam Fatehmanesh, Matt Thomson, James Gornet, David Prober",Spotlight
Learning Time-Scale Invariant Population-Level Neural Representations,"General-purpose foundation models for neural time series can help accelerate neuroscientific discoveries and enable applications such as brain computer interfaces (BCIs). A key component in scaling these models is population-level representation learning, which leverages information across channels to capture spatial as well as temporal structure. Population-level approaches have recently shown that such representations can be both efficient to learn on top of pretrained temporal encoders and produce useful representations for decoding a variety of downstream tasks. However, these models remain sensitive to mismatches in preprocessing, particularly on time-scales, between pretraining and downstream settings. We systematically examine how time-scale variability affects generalization and find that existing representations lack invariance. To address this, we introduce Time-scale Augmented Pretraining (TSAP), which consistently improves robustness to different time-scales across decoding tasks and builds invariance in the representation space. These results highlight handling preprocessing diversity as a key step toward building generalizable neural foundation models.",https://openreview.net/forum?id=gPw0qPn908,"Eshani Patel, Yisong Yue, Geeling Chau",Poster
Towards Clinically Faithful ECG Reports via Quantization-Based Tokenization,"Effective tokenization is a critical barrier to bridging continuous electrocardiogram (ECG) signals and discrete language models for automated report generation. We introduce a novel ECG tokenizer based on an adaptive residual vector quantization framework, QINCo, that learns a high-fidelity, discrete representation of raw 12-lead signals. The clinical utility of these tokens is demonstrated through a downstream classification task, where their frozen embeddings achieve performance comparable to specialized supervised and self-supervised methods. Leveraging this tokenizer with an attention-based adapter, our approach to report generation outperforms byte-level tokenizer baselines and establishes a benchmark on a large-scale clinical dataset. Our work presents an effective and computationally efficient tokenization framework, enabling a more powerful integration of complex biosignals into generative models for clinical applications.",https://openreview.net/forum?id=MJCDK4mnAa,"Rohan Banerjee, Jacques Delfrate, Robert Avram",Poster
Processing fMRI Brain Signals Using Latents from Natural Image Autoencoders,"Modeling long-range spatiotemporal dynamics in functional Magnetic Resonance Imaging (fMRI) remains a key challenge due to the high dimensionality of the four-dimensional signals. Prior voxel-based models, although demonstrating excellent performance and interpretation capabilities, are constrained by prohibitive memory demands and thus can only capture limited temporal windows. To address this, we propose TABLeT (Two-dimensionally Autoencoded Brain Latent Transformer), a novel approach that tokenizes fMRI volumes using a pre-trained 2D natural image autoencoder. Each 3D fMRI volume is compressed into a compact set of continuous tokens, enabling efficient long-sequence modeling with a simple transformer encoder. Across large-scale benchmarks including the Human Connectome Project (HCP) and ADHD-200 datasets, TABLeT consistently outperforms existing models in multiple tasks, while demonstrating substantial gains in computational and memory efficiency over the state-of-the-art voxel-based method. Our findings highlight a new paradigm for scalable spatiotemporal modeling of brain activity.",https://openreview.net/forum?id=8qXOE72CNh,"Juhyeon Park, Peter Yongho Kim, Jungwoo Park, Jubin Choi, Jungwoo Seo, Jiook Cha, Taesup Moon",Spotlight
MultiDiffNet: A Multi-Objective Diffusion Framework for Generalizable Brain Decoding,"Neural decoding from electroencephalography (EEG) remains fundamentally limited by poor generalization to unseen subjects, driven by high inter-subject variability and the lack of large-scale datasets to model it effectively. Existing methods often rely on synthetic subject generation or simplistic data augmentation, but these strategies fail to scale or generalize reliably. We introduce MultiDiffNet, a diffusion-based framework that bypasses generative augmentation entirely by learning a compact latent space optimized for multiple objectives. We decode directly from this space and achieve state-of-the-art generalization across various neural decoding tasks using subject and session disjoint evaluation. We also curate and release a unified benchmark suite spanning four EEG decoding tasks of increasing complexity (SSVEP, Motor Imagery, P300, and Imagined Speech) and an evaluation protocol that addresses inconsistent split practices in prior EEG research. Finally, we develop a statistical reporting framework tailored for low-trial EEG settings. Our work provides a reproducible and open-source foundation for subject-agnostic EEG decoding in real-world BCI systems.",https://openreview.net/forum?id=scpBMrMSMt,"Meng-Chun Zhang, Kateryna Shapovalenko, Yucheng Shao, Yuzhi Guo, Parusha Pradhan",Poster
Towards Interpretable Visual Decoding with Attention to Brain Representations,"Recent work has demonstrated that complex visual stimuli can be decoded from human brain activity using deep generative models. However, most current approaches rely on mapping brain data into intermediate image or text feature spaces before guiding the generative process, masking the effect of responses from different brain areas on the final reconstruction output. In this work, we propose \textit{NeuroAdapter}, a framework that directly conditions a latent diffusion model on brain representations, bypassing the need for intermediate feature spaces. Our method demonstrates competitive visual reconstruction quality on the Natural Scenes Dataset (NSD). To reveal how different cortical areas influence the unfolding generative trajectory, we contribute an Image–Brain BI-directional interpretability framework (\textit{IBBI}) which investigates cross attention mechanisms across diffusion steps. Our work enables new approaches for interpreting latent diffusion models through the lens of visual processing in the brain.",https://openreview.net/forum?id=Pes6WGkeV6,"Pinyuan Feng, Hossein Adeli, Wenxuan Guo, Ethan Hwang, Fan L. Cheng, Nikolaus Kriegeskorte",Poster
PhysioJEPA: Joint Embedding Representations of Physiological Signals for Real Time Risk Estimation in the Intensive Care Unit,"Self-supervised learning of multichannel, high-frequency physiological signals is largely unexplored, despite their potential for critical care applications. We present PhysioJEPA, the first joint embedding predicting architecture (JEPA) designed for multichannel physiological signals from critical care bedside monitoring devices. PhysioJEPA learns representations from 30-minute segments of physiological signals from three channels: arterial blood pressure, electrocardiography lead II, and photoplethysmography, using the MIMIC-III Waveform Database. Trained on over 9.6 million minutes of arterial blood pressure, electrocardiography lead II, and photoplethysmography from 4,134 intensive care unit stays, PhysioJEPA's learned representations can be finetuned to estimate 5-minute risk of hypotension (AUROC = 0.86) and shock index (AUROC = 0.93) improving performance over a supervised convolutional baseline (AUROC = 0.83 and 0.92 for hypotension and shock, respectively). Furthermore, it generalized to an external cohort from the Mount Sinai Health System (AUROC = 0.79 and 0.88). These results suggest that self-supervised representation learning is a promising approach for critical care signal data.",https://openreview.net/forum?id=JHMLr8zRuZ,"Benjamin Fox, Dung T. Hoang, ANKIT PAREKH, Girish N Nadkarni, Ankit Sakhuja",Poster
Leveraging Foundational Models and Simple Fusion for Multi-modal Physiological Signal Analysis,"Physiological signals such as electrocardiograms (ECG) and electroencephalograms (EEG) provide complementary insights into human health and cognition, yet multi-modal integration is challenging due to limited multi-modal labeled data, and modality-specific differences . In this work, we adapt the CBraMod encoder for large-scale self-supervised ECG pretraining, introducing a dual-masking strategy to capture intra- and inter-lead dependencies. To overcome the above challenges, we utilize a pre-trained CBraMod encoder for EEG and pre-train a symmetric ECG encoder, equipping each modality with a rich foundational representation. These representations are then fused via simple embedding concatenation, allowing the classification head to learn cross-modal interactions, together enabling effective downstream learning despite limited multi-modal supervision. Evaluated on emotion recognition, our approach achieves near state-of-the-art performance, demonstrating that carefully designed physiological encoders, even with straightforward fusion, substantially improve downstream performance. These results highlight the potential of foundation-model approaches to harness the holistic nature of physiological signals, enabling scalable, label-efficient, and generalizable solutions for healthcare and affective computing.",https://openreview.net/forum?id=qTVtaG7R7I,"Youssef Ghallab, Omar Iraqy, Mohamed Kandil, Mohamed Ashraf, Saad El Dine Ahmed, Morougue Ghazal, Ayman Khalafallah, Nagwa El-Makky",Poster
Adapting Neural Audio Codecs to EEG,"EEG and audio are inherently distinct modalities, differing in sampling rate, channel structure, and scale. Yet, we show that pretrained neural audio codecs can serve as effective starting points for EEG compression, provided that the data are preprocessed to be suitable to the codec’s input constraints. Using DAC, a state-of-the-art neural audio codec as our base, we demonstrate that raw EEG can be mapped into the codec’s stride-based framing, enabling direct reuse of the audio-pretrained encoder-decoder. Even without modification, this setup yields stable EEG reconstructions, and fine-tuning on EEG data further improves fidelity and generalization compared to training from scratch. We systematically explore compression-quality trade-offs by varying residual codebook depth, codebook (vocabulary) size, and input sampling rate. To capture spatial dependencies across electrodes, we propose DAC-MC, a multi-channel extension with attention-based cross-channel aggregation and channel-specific decoding, while retaining the audio-pretrained initialization. Evaluations on the TUH Abnormal and Epilepsy datasets show that the adapted codecs preserve clinically relevant information, as reflected in spectrogram-based reconstruction loss and downstream classification accuracy.",https://openreview.net/forum?id=i4Vf28vvhQ,"Ard Kastrati, Luca A Lanzendörfer, Riccardo Rigoni, John Staib Matilla, Roger Wattenhofer",Poster
EEG-Bench: A Benchmark for EEG Foundation Models in Clinical Applications,"We introduce a unified benchmarking framework focused on evaluating EEG-based foundation models in clinical applications. The benchmark spans 11 well-defined diagnostic tasks across 14 publicly available EEG datasets, including epilepsy, schizophrenia, Parkinson’s disease, OCD, and mild traumatic brain injury. It features minimal preprocessing, standardized evaluation protocols, and enables side-by-side comparisons of classical baselines and modern foundation models. Our results show that while foundation models achieve strong performance in certain settings, simpler models often remain competitive, particularly under clinical distribution shifts. To facilitate reproducibility and adoption, we release all prepared data and code in an accessible and extensible format.",https://openreview.net/forum?id=MAudehqShe,"Ard Kastrati, Josua Bürki, Jonas Lauer, Cheng Xuan, Raffaele Iaquinto, Roger Wattenhofer",Poster
GAN-Guided Diffusion Models for Generating Clinically Meaningful Multimodal Neuroimaging Data,"Multimodal brain imaging provides complementary insights into brain structure and function, but its capability is often limited by missing modalities. Traditional imputation and subsampling strategies are computationally simple, but have the risk of introducing bias or discarding valuable samples. Recently, generative models have emerged as powerful alternatives for synthesizing missing modalities. In this study, we introduce a GAN-guided diffusion framework for cross-modality translation, designed to generate both T1-weighted MRI and functional network connectivity (FNC) data. The framework integrates conditional diffusion modeling, adversarial learning, and cycle-consistency, enabling training with both paired and unpaired samples. On Alzheimer’s disease data, our approach outperformed baseline methods, achieving higher peak signal-to-noise ratio (PSNR) (24.95) and structural similarity index measure (SSIM) (0.86) for T1 synthesis, as well as improved correlation with real FNCs (0.65). Furthermore, our results demonstrate that the model captures variability across clinical groups without supervision from diagnostic labels, producing realistic and clinically meaningful synthetic modalities for downstream analysis and biomarker discovery.",https://openreview.net/forum?id=luatL3pppl,"Reihaneh Hassanzadeh, Anees Abrol, Hamid Hassanzadeh, Vince D. Calhoun",Poster
"Multi-modal, multi-species, and multi-task latent-space model for decoding level of consciousness","Assessing degree and characteristic of consciousness is central to caring for patients with disorders of consciousness (DoC), yet current standard of care is a bedside questionnaire. Using a laminar neural probe we introduce a self-supervised, multi-modal representation learning approach that constructs an interpretable 2-D latent space of brain state from continuous neural recordings. We jointly encode local field potentials (LFPs), single-unit firing rates, and a movement proxy into a time-aware autoregressive VAE (TN-VAE). In rats undergoing controlled depth anesthesia experiments, the learned latent trajectories nonlinearly, but smoothly, separate four expert-defined states (awake, light, moderate, deep) at $2s$ resolution, exceeding the temporal granularity of behavioral scoring ($15m$). The latent space supports linear readout of both coarse state and individual behavioral components, and its axes align with known physiology: delta/alpha power differentiates unconscious sub-states, while gamma and unit firing distinguish wakefulness. To enable cross-species use and incomplete modality sets, we add lightweight, species-specific stitching layers. This model pretrained on tri-modal rat data and fine-tuned with unimodal (LFP) human intraoperative data successfully separates awake versus anesthetized states in new unseen human subjects, demonstrating zero-shot multi-subject transfer without per-patient calibration. The model further generalizes in a multi-task manner to predict the results of additional stimuli. These results highlight a path toward a foundation model for DoC that generalizes across sessions, subjects, species, and stimuli to enable scalable, continuous brain-state monitoring and clinical decision support.",https://openreview.net/forum?id=fk2hDc9DyL,"Julia Huiming Wang, Méliya El Fakiri, Jake Swann, Elena Gado, Scott Cheng, Jordy Tasserie, Rongchen Huang, Jongha Lee, Axoft, Jia Liu, Tianyang Ye, Paul Le Floch, Oliver Armitage",Poster
∆ DELTA: Language Diffusion-based EEG-to-Text Architecture,"Since electroencephalograms (EEG) are high-dimensional, noisy, and continuous signals, the EEG-to-text task of decoding them into text has been limited in performance by channel-specific noise, individual differences, teacher-forcing dependence, and cumulative errors in sequential prediction. To address these limitations, we propose DELTA, a framework that combines a Residual Vector Quantization (RVQ)-based EEG tokenizer with a Large Language Diffusion with mAsking (LLaDA). RVQ converts continuous EEG signals into multi-layer discrete tokens to mitigate noise and individual differences, while LLaDA performs a non-sequential denoising process on the tokens to recover sentences. In experiments on the ZuCo dataset, DELTA improves the semantic fit by up to 5.37 percentage points over traditional autoregressive models, achieving BLEU-1 scores of 21.9 and ROUGE-1 F scores of 17.2 under word-level conditions. The proposed method enables reliable natural language generation from small-scale EEG-text data and is expected to be extended to multimodal EEG-language models in the future.",https://openreview.net/forum?id=Hw3wWRyLs4,"mingyu jeon, Hyobin Kim",Poster
Interpretable-MTLNet: A Kolmogorov–Arnold Network for Multitask Mental Health Prediction,"Interpretable-MTLNet is a multitask neural architecture that jointly detects depression and anxiety from daily wearable time series while exposing mathematically transparent relations. We couple multi-scale temporal convolutions with Kolmogorov–Arnold Network (KAN) layers to learn scale-aware embeddings and task-specific spline-based heads. This design yields consistent global and local explanations via activation-weighted univariate curves and symbolic surrogates. Evaluated on 40k participants with linked Fitbit data from the All of Us Research Program, Interpretable-MTLNet achieves a macro-AUROC of 0.731 across tasks, outperforming strong baselines under subject-level splits, with robust gains in AUROC in imbalanced settings. Our results indicate that KANs can achieve accuracy and interpretability for detecting mental health problems from wearables.",https://openreview.net/forum?id=xypzPCMBah,"Mai Ali, Zhenghao Ni, Deepa Kundur",Poster
Efficient Calibration in Motor Imagery BCIs Under Data Constraints via Subject Transfer,"The practical deployment of motor imagery brain-computer interfaces (MI-BCIs) is bottlenecked by their dependence on large amounts of subject-specific calibration data. To overcome this, we introduce Riemannian Transfer CSP (RTCSP), a transfer learning method that aligns EEG covariance matrices from previous users to a new target subject using Riemannian geometry. This process generates robust spatial filters that perform well even when the target subject's calibration data is severely limited. Our method enables efficient and accurate calibration with minimal data, directly addressing a key challenge for practical MI-BCI systems.",https://openreview.net/forum?id=MQNOt7fCiH,"Tekin Gunasar, Virginia R. de Sa",Poster
‘EEGReXferNet’ – A Lightweight Gen-AI Framework for EEG Subspace Reconstruction via Cross-Subject Transfer Learning and Channel-Aware Embedding.,"Electroencephalography (EEG) is a widely used non-invasive technique for monitoring brain activity, but low signal to-noise ratios (SNR) due to various artifacts often compromise its utility. Conventional artifact removal methods require manual intervention or risk suppressing critical neural features during filtering/reconstruction. Recent advances in generative models, including Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs), have shown promise for EEG reconstruction; however, these approaches often lack integrated temporal spectral- spatial sensitivity and are computationally intensive, limiting their suitability for real-time applications like brain–computer interfaces (BCIs). To overcome these challenges, we introduce EEGReXferNet, a lightweight Gen-AI framework for EEG subspace reconstruction via cross-subject transfer learning - developed using Keras TensorFlow (v2.15.1). EEGReXferNet employs a modular architecture that leverages volume conduction across neighboring channels, band-specific convolution encoding, and dynamic latent feature extraction through sliding windows. By integrating reference-based scaling, the framework ensures continuity across successive windows and generalizes effectively across subjects. This design im- proves spatial-temporal-spectral resolution (mean PSD correlation $\geq$ 0.94; mean spectrogram RV-Coefficient $\geq$ 0.85), reduces total weights by $\sim45\\%$ to mitigate overfitting, and maintains computational efficiency for robust, real-time EEG preprocessing in neurophysiological and BCI applications.",https://openreview.net/forum?id=TOW1by49Ec,"Shantanu Sarkar, Piotr Nabrzyski, Saurabh Prasad, Jose L Contreras-Vidal",Poster
Imitation learning of dexterous hand control uncovers muscle-level representations in primate sensorimotor cortex,"Dexterous grasping requires the integration of proprioceptive feedback with predictive motor commands. Yet, how cortical circuits combine afferent feedback with efference signals during grasp remains poorly understood. Here we combine deep reinforcement learning, biomechanics, and neural recordings to build a closed-loop, muscle-level controller of primate grasping. A neural network policy, trained via imitation learning on a 39-muscle musculoskeletal hand, reproduces naturalistic pre-contact shaping and develops internal states that explain single-neuron activity in primary motor (M1) and somatosensory (S1) cortex. Three principles emerged. First, muscle-based control aligns more closely with cortical dynamics than joint-based control, even when the latter achieves higher tracking accuracy. Second, task optimization drives the emergence of internal representations that capture both trial-to-trial variability and object-specific tuning. Third, trajectory embeddings learned within the network can be decoded from M1 to directly drive the controller with only tens of neurons, outperforming joint-angle decoding. Together, these findings establish a stimulus-computable framework that reveals integrated, muscle-centric and goal-latent codes in S1/M1 while opening a novel route for creating brain-body models.",https://openreview.net/forum?id=iNSQcb4W86,"Alessandro Marin Vargas, Adriana Perez Rotondo, Alberto Silvio Chiappa, Mackenzie W Mathis, Alexander Mathis",Poster
Mitigating Subject Dependency in EEG Decoding with Subject-Specific Low-Rank Adapters,"Subject-specific distribution shifts represent an important obstacle to the development of foundation models for EEG decoding. To address this, we propose Subject-Conditioned Layer, an adaptive layer designed as a drop-in replacement for standard linear or convolutional layers in any neural network architecture. Our layer captures subject-specific variability by decomposing its weights into a shared, subject-invariant component and a lightweight, low-rank correction unique to each subject. This explicit separation of general knowledge from personalized adaptation allows existing models to become robust to subject shifts. Empirically, models equipped with our layer outperform both a shared-weight-only model (subject-agnostic model) and the average of individually trained subject-specific models. Consequently, the Subject-Conditioned Layer offers a practical and scalable path towards building effective cross-subject foundation models for EEG.",https://openreview.net/forum?id=vDtsuVfBmG,"Timon Klein, Piotr Minakowski, Sebastian Sager",Spotlight
Optimizing fMRI Data Acquisition for Decoding Natural Speech with Limited Participants,"We present a systematic investigation into decoding perceived natural speech from fMRI data in a participant-limited setting. Using a publicly available dataset of eight participants (LeBel et al., 2023), we demonstrate that deep neural networks trained with a contrastive objective can effectively decode unseen natural speech by retrieving perceived sentences from fMRI activity. We found that decoding performance directly correlates with the amount of training data available per participant. In this data regime, multi-subject training does not improve decoding accuracy compared to the single-subject approach. Additionally, training on similar or different stimuli across subjects has a negligible effect on decoding accuracy. Finally, we find that our decoders model both syntactic and semantic features, and that stories containing sentences with complex syntax or rich semantic content are more challenging to decode. While our results demonstrate the benefits of having extensive data per participant (deep phenotyping), they suggest that leveraging multi-subject data for natural speech decoding likely requires deeper phenotyping or a substantially larger cohort.",https://openreview.net/forum?id=aWQFT2V3eI,"Louis Jalouzot, Alexis Thual, Yair Lakretz, Christophe Pallier, Bertrand Thirion",Poster
Not All Sessions Are Equal: Data Selection for Multi-Session Pretraining in Neural Data Transformers,"A key challenge in analyzing neuroscience datasets is the profound variability they exhibit across sessions, animals, and data modalities. Several recent studies have demonstrated performance gains from pretraining neural foundation models on multi-session datasets, seemingly overcoming this challenge. However, these studies typically lack fine-grained data scaling analyses. It remains unclear whether all sessions contribute equally to downstream performance gains. In this work, we systematically investigate how cross-session variability impacts the scaling behavior of neural data transformers (NDTs) in neural activity prediction. We propose a session selection procedure based on single-session finetuning performances. Through this procedure, models pretrained on as few as five selected sessions outperformed those pretrained on the entire dataset of 84 sessions. Our findings challenge the direct applicability of traditional scaling laws to neural data and suggest that multi-session scaling benefits may need to be re-examined in the light of session-to-session variability. This work both highlights the importance of incremental data scaling analyses and suggests new avenues toward optimally selecting pretraining data when developing foundation models on large-scale neuroscience datasets.",https://openreview.net/forum?id=Due0ZtLZiS,"Linxing Preston Jiang, Shirui Chen, Emmanuel Tanumihardja, Xiaochuang Han, Weijia Shi, Eric Todd SheaBrown, Rajesh P. N. Rao",Poster
A foundation model with multi-variate parallel attention to generate neuronal activity,"Learning from multi-variate time-series with heterogeneous channel configurations remains a fundamental challenge for deep neural networks, particularly in clinical domains such as intracranial electroencephalography (iEEG), where channel setups vary widely across subjects. In this work, we introduce multi-variate parallel attention (MVPA), a novel self-attention mechanism that disentangles content, temporal, and spatial attention, enabling flexible, generalizable, and efficient modeling of time-series data with varying channel counts and configurations. We use MVPA to build MVPFormer, a generative foundation model for human electrophysiology, trained to predict the evolution of iEEG signals across diverse subjects. To support this and future efforts by the community, we release the Long-term iEEG dataset, the largest publicly available iEEG dataset to date, comprising nearly 10,000 hours of recordings from heterogeneous clinical sources. MVPFormer leverages MVPA to achieve strong generalization across subjects, demonstrating expert-level performance in several iEEG tasks. MVPFormer surpasses state-of-the-art (SOTA) Transformer baselines in seizure detection across the Long-term, the MAYO, and the FNUSA dataset, while also achieving SOTA performance on four Brain TreeBank iEEG decoding tasks (volume, pitch, onset, and speech). Together, our contributions establish MVPFormer as the first open-source, open-weights, and open-data iEEG foundation model with SOTA clinical performance.",https://openreview.net/forum?id=hFpCdq5QNl,"Francesco S. Carzaniga, Michael Hersche, Abu Sebastian, Kaspar Schindler, Abbas Rahimi",Poster
Are foundation models useful feature extractors for electroencephalography analysis?,"The success of foundation models in natural language processing and computer vision has motivated similar approaches in time series analysis. While foundational time series models have proven beneficial on a variety of tasks, their effectiveness in medical applications with limited data remains underexplored. In this work, we investigate this question in the context of electroencephalography (EEG) by evaluating general-purpose time series models on age prediction, seizure detection, and classification of clinically relevant EEG events. We compare their diagnostic performance against specialised EEG models and assess the quality of the extracted features. The results show that general-purpose models are competitive and capture features useful to localising demographic and disease-related biomarkers. These findings indicate that foundational time series models can reduce the reliance on large task-specific datasets and models, making them valuable in clinical practice.",https://openreview.net/forum?id=AwxomC5U5a,"Özgün Turgut, Felix S. Bott, Markus Ploner, Daniel Rueckert",Poster
CONFORM: A Project to Create Crowd-Sourced Open Neuroscience fMRI Foundation Models,"We propose CONFORM (Crowd-Sourced Open Neuroscience fMRI Foundation Model), a project that will bring together recent advances in neural data processing and analysis with a novel, crowd-sourced infrastructure. This transformative approach will overcome several current challenges in creating a foundational human fMRI model for vision: collecting massive amounts of data from a handful of participants is neither scalable nor sustainable; the number of participants is small for such datasets; stimulus diversity is limited; and generalizability to different populations is poor. CONFORM will overcome these limitations by combining a powerful generative denoising method (SNAP), a scalable framework for aggregating existing fMRI datasets (MOSAIC), and a meta-learning model that enables generalization with much smaller data from new participants (BraInCoRL). Our collaborative effort will produce models built on unprecedented scale and diversity—ultimately with hundreds of participants and hundreds of thousands of naturalistic image and movie stimuli—and provide the tools for continuous expansion of the underlying dataset. This ``crowd-sourced'' approach will allow many more researchers to leverage state-of-the-art NeuroAI methods using the scale of data they typically collect, democratizing access to powerful models and accelerating scientific discovery for a wide range of neuroscientific domains and populations.",https://openreview.net/forum?id=3STtCcHs7i,"Michael J. Tarr, Jacob S. Prince, Ben Lahner, Mayukh Deb, Aude Oliva, Apurva Ratan Murty, John A. Pyles, Margaret Marie Henderson, Leila Wehbe, Andrew Luo",Poster
Learning the relative composition of EEG signals using pairwise relative shift pretraining,"Self-supervised learning (SSL) offers a promising approach for learning electroencephalography (EEG) representations from unlabeled data, reducing the need for expensive annotations for clinical applications like sleep staging and seizure detection. While current EEG SSL methods predominantly use masked reconstruction strategies like masked autoencoders (MAE) that capture local temporal patterns, position prediction pretraining remains underexplored despite its potential to learn long-range dependencies in neural signals. We introduce PAirwise Relative Shift or PARS pretraining, a novel pretext task that predicts relative temporal shifts between randomly sampled EEG window pairs. Unlike reconstruction-based methods that focus on local pattern recovery, PARS encourages encoders to capture relative temporal composition and long-range dependencies inherent in neural signals. Through comprehensive evaluation on various EEG decoding tasks, we demonstrate that PARS-pretrained transformers consistently outperform existing pretraining strategies in label-efficient and transfer learning settings, establishing a new paradigm for self-supervised EEG representation learning.",https://openreview.net/forum?id=kk8EUOiILZ,"Christopher Michael Sandino, Sayeri Lala, Geeling Chau, Melika Ayoughi, Juri Minxha, Ellen L. Zippi, Ali Moin, Behrooz Mahasseni, Erdrin Azemi, Hanlin Goh",Poster
EEG Foundation Models: A Critical Review of Current Progress and Future Directions,"Electroencephalography (EEG) signals offer immense value in scientific and clinical investigations. In recent years, self-supervised EEG foundation models (EEG-FMs) have presented a viable path towards the robust and scalable extraction of EEG features. However, the real-world readiness of early EEG-FMs and the rubrics for long-term research progress remain unclear. This study conducts a critical review of ten early, first-generation EEG-FMs based on a) the representation of raw input data, b) self-supervised representation learning, and c) evaluation strategy. We synthesize key EEG-FM methodological trends, empirical findings, and remaining gaps. We find that EEG-FMs draw heavily from their counterparts in the language and vision domains for their model architecture and self-supervision. However, EEG-FM evaluations remain heterogeneous and largely limited, making it challenging to assess their practical off-the-shelf utility. In addition to adopting standardized and realistic evaluations, future efforts should demonstrate substantial scaling effects and make principled and trustworthy choices throughout the EEG-FM pipeline. We believe that the development of benchmarks, software tools, technical methodologies, and clinical/scientific applications in collaboration with domain experts may advance real-world adoption of EEG-FMs.",https://openreview.net/forum?id=Iu6qVgtgUD,"Gayal Kuruppu, Neeraj Wagh, Yogatheesan Varatharajah",Poster
Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model,"The emergence of foundation models in neuroimaging is driven by the increasing availability of large-scale and heterogeneous brain imaging datasets. Recent ad- vances in self-supervised learning, particularly reconstruction-based objectives, have demonstrated strong potential for pretraining models that generalize effec- tively across diverse downstream functional MRI (fMRI) tasks. In this study, we ex- plore region-aware reconstruction strategies for a foundation model in resting-state fMRI, moving beyond approaches that rely on random region masking. Specifically, we introduce an ROI-guided masking strategy using the Automated Anatomical Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively mask semantically coherent brain regions during self-supervised pretraining. Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI scans, we show that our method achieves a 4.23% improvement in classification accu- racy compared to conventional random masking. Region-level attribution analysis reveals that brain volumes within the limbic region and cerebellum contribute most significantly to reconstruction fidelity and model representation. Our results demonstrate that masking anatomical regions during model pretraining not only enhances interpretability but also yields more robust and discriminative representa- tions. In future work, we plan to extend this approach by evaluating it on additional neuroimaging datasets, and developing new loss functions explicitly derived from region-aware reconstruction objectives. These directions aim to further improve the robustness and interpretability of foundation models for functional neuroimaging.",https://openreview.net/forum?id=52HyCgZrgv,"Ruthwik Reddy Doodipala, Pankaj Pandey, Carolina Torres Rojas, Manob Jyoti Saikia, Ranganatha Sitaram",Poster
Predictive Modeling of Brain-Body Association,"The brain senses and regulates internal organs through a process called interoception. Appealing theoretical frameworks have been proposed to describe this dynamic interaction, yet they lack computational models that are fully computable with experimental data. In this paper, we develop a foundation model to substantiate the embodied predictive interoception coding theory. Through self-supervised learning, the model learns the dynamics of the physiological data and captures their association with brain activities. The model reveals a bilateral and distributed brain network across subcortical to cortical levels that encodes respiratory and cardiac dynamics and predicts physiological states forward by seconds to tens of seconds. As an initial attempt, our work merits future studies in modeling interoception with concurrent brain and body measurements.",https://openreview.net/forum?id=h1Ev5DCbgX,"Minkyu Choi, Xiaokai Wang, Zhongming Liu",Poster
CPEP: Contrastive Pose-EMG Pre-training Enhances Gesture Generalization on EMG signals,"Hand gesture classification using high-quality structured data such as videos, images, and hand skeletons is a well-explored problem in computer vision. Leveraging low-power, cost-effective biosignals, e.g. surface electromyography (sEMG), allows for continuous gesture prediction on wearables. In this paper, we demonstrate that learning representations from weak-modality data that are aligned with those from structured, high-quality data can improve representation quality and enables zero-shot classification.Specifically, we propose a Contrastive Pose-EMG Pre-training (CPEP) framework to align EMG and pose representations, where we learn an EMG encoder that produces high-quality and pose-informative representations. We assess the gesture classification performance of our model through linear probing and zero-shot setups. Our model outperforms emg2pose benchmark models by up to 21% on in-distribution gesture classification and 72% on unseen (out-of-distribution) gesture classification.",https://openreview.net/forum?id=cr7jItQVAN,"Wenhui Cui, Christopher Michael Sandino, Hadi Pouransari, Ran Liu, Juri Minxha, Ellen L. Zippi, Aman Verma, Anna Sedlackova, Behrooz Mahasseni, Erdrin Azemi",Spotlight
Brain2Model Learning: Training sensory and decision models with human neural activity as a teacher,"Cognitive neuroscience shows that the human brain creates low-dimensional, abstract representations for efficient sensorimotor coding. Importantly, the brain can learn these representations with significantly fewer data points and less computational power than artificial models require. We introduce Brain2Model Learning (B2M), a framework where neural activity from human sensory and decision-making guides the training of artificial neural networks, via contrastive learning or latent regression. We provide a proof-of-concept for B2M in memory-based decision-making with a recurrent neural network and scene reconstruction for autonomous driving with a variational autoencoder. Our results show that student networks benefiting from brain-derived guidance can either converge faster, achieve higher predictive accuracy, or both, compared to networks trained in isolation. This indicates that the brain's representations can be useful for artificial learners, facilitating efficient learning of sensorimotor representations, which would be costly or slow through purely artificial training.",https://openreview.net/forum?id=iV6UJyFrjH,"Tomas Aquino, Victoria Qinwanxian Liu, Habiba Azab, Raissa Mathura, Andrew J Watrous, Eleonora Bartoli, Benjamin Y Hayden, Paul Sajda, Sameer A. Sheth, Nuttida Rungratsameetaweemana",Poster
Towards foundation models of naturalistic collective social‑neural dynamics,"Foundation models for the brain and body (FMBB) can transform neuroscience by improving generalization, standardization and reproducibility. However, building generally useful FMBB requires large-scale, high-quality datasets, which capture the immense complexity and variability of naturalistic interactions. Unfortunately, measuring neural activity during such interactions is extremely challenging, especially with high spatiotemporal resolution and in human subjects. Therefore, we have collected a large and unique dataset comprising 17 groups of three-four mice, freely interacting in an enriched environment under continuous video monitoring for one week. We used wireless neural loggers to electrophysiologically record medial prefrontal cortex (mPFC) activity with single-spike, single-unit resolution from all the group members simultaneously, and we systematically perturbed this neural activity using wireless optogenetics to measure its behavioral effects. To study different levels of behavior and their neural representations, we established an extensive, carefully curated and highly accurate preprocessing pipeline, including spike sorting, 3D pose estimation, interpretable behavioral feature extraction, high‑level behavior classification, and social dominance hierarchy (SDH) extraction. We then trained foundation models using self‑supervised representation learning with CEBRA on the behavioral data, pooled across sessions and animals. We devised a custom method of feature partitioning to make the contrastive learning task more challenging and show that using the learned embeddings instead of the original features as inputs to downstream models trained to predict neural activity significantly improves their performance. We then use feature attribution methods to show how this can complement classical analysis of neural tuning. Taken together, this work paves a path towards building generally useful FMBB during naturalistic social interactions.",https://openreview.net/forum?id=DIL6fJ9427,"Asaf Benjamin, Sergey Anpilov, Omer Izhaki, Maya Sheffer, Tommaso Biagini, Yair Shemesh, Alon Rubin, Inbar Saraf-Sinik, Mackenzie W Mathis, Alexander Mathis, Alon Chen, Ofer Yizhar",Poster
FireGNN: Neuro-Symbolic Graph Neural Networks with Trainable Fuzzy Rules for Interpretable Medical Image Classification,"Medical image classification requires not only high predictive performance but also interpretability to ensure clinical trust and adoption. Graph Neural Networks (GNNs) offer a powerful framework for modeling relational structures within datasets, however, standard GNNs often operate as black boxes, limiting transparency and usability particularly in clinical settings. In this work, we present an interpretable graph-based learning framework named FireGNN that integrates trainable fuzzy rules into GNNs for medical image classification. These rules embed topological descriptors - node degree, clustering coefficient, and label agreement - using learnable thresholds and sharpness parameters to enable intrinsic symbolic reasoning. Additionally, we explore auxiliary self-supervised tasks (e.g., homophily prediction, similarity entropy) as a benchmark to evaluate the contribution of topological learning. Our fuzzy-rule-enhanced model achieves strong performance across five MedMNIST benchmarks and the synthetic dataset MorphoMNIST, while also generating interpretable rule-based explanations. To our knowledge, this is the first integration of trainable fuzzy rules within a GNN.",https://openreview.net/forum?id=y3dhKusYFx,"Prajit Sengupta, Islem Rekik",Poster
Latent Graph Learning in Generative Models of Neural Signals,"Inferring temporal interaction graphs and higher-order structure from neural signals is a key problem in building generative models for systems neuroscience. Foundation models for large-scale neural data represent shared latent structures of neural signals. However, extracting interpretable latent graph representations in foundation models remains challenging and unsolved. Here we explore latent graph learning in generative models of neural signals. By testing against numerical simulations of neural circuits with known ground-truth connectivity, we evaluate several hypotheses for explaining learned model weights. We discover modest alignment between extracted network representations and the underlying directed graphs and strong alignment in the co-input graph representations. These findings motivate paths towards incorporating graph-based geometric constraints in the construction of large-scale foundation models for neural data.",https://openreview.net/forum?id=DL5KIUii86,"Nathan Kodama, Kenneth A. Loparo",Poster
Simple Temporal Attention Beats Complex Decoders for Neural-to-Visual Mapping from Primate Spiking Data,"Understanding how neural activity gives rise to perception remains a fundamental challenge in neuroscience. Here, we address the problem of visual decoding from high-density intracortical recordings in primates using the THINGS Ventral Stream Spiking Dataset. We systematically evaluate the effects of model architecture, loss function, and temporal aggregation, showing that decoding accuracy is primarily driven by temporal dynamics rather than architectural complexity. A lightweight model combining temporal attention with a shallow MLP achieves up to 70% top-1 image retrieval accuracy, outperforming linear and recurrent baselines. Building on this, we introduce a modular generative pipeline that combines low-resolution latent reconstruction with semantically guided diffusion. By generating and ranking multiple candidate images via rejection sampling, our approach enables photorealistic reconstructions from 200 ms of brain activity. These results provide actionable insights for neural decoding and establish a flexible framework for future brain–computer interfaces and semantic reconstruction from brain signals.",https://openreview.net/forum?id=zGZ7MjffUR,"Matteo Ciferri, Matteo Ferrante, Nicola Toschi",Poster
Representation-First Emotion Decoding from 7t fMRI,"We present a scalable signal processing framework for measuring naturalistic emotional responses in 7T fMRI using 3D convolutional neural networks. Our model learns low-dimensional representations of affective activity from whole-brain recordings during narrative-driven auditory stimulation, recovering structure consistent with valence–arousal dimensions in affective neuroscience. By prioritizing emotional representation learning over anatomical interpretability, the model maps neural activity across individuals into a shared latent space aligned with canonical affective geometry, enabling scalable cross-subject analysis without region-specific assumptions. We also observe subject-specific deviations in these representations that may capture individual differences in emotional processing, suggesting opportunities for downstream interpretation and personalized analysis. This work establishes a scalable deep learning framework for emotion-aware representation learning in fMRI.",https://openreview.net/forum?id=axphVwbQYx,"Joshua Lunger, Jurgen Germann",Poster
ENIGMA: A Unified Lightweight EEG-to-Image Model for Multi-Subject Visual Decoding,"To be practical for real-life applications, models for reconstructing seen images from human brain activity must be effective on affordable scanning hardware, small enough to run locally on accessible computing resources, and easily and consistently deployable across multiple subjects in downstream tasks. To directly address these current limitations, we introduce ENIGMA, a multi-subject electroencephalography (EEG)-to-Image decoding model that reconstructs seen images from EEG recordings and achieves state-of-the-art (SOTA) performance on the research-grade THINGS-EEG2 and consumer-grade AllJoined-1.6M benchmarks. ENIGMA boasts a simpler architecture and has ~120x fewer parameters than previous SOTA methods, integrating a set of subject-specific encoder layers with a subject-unified spatio-temporal backbone to map raw EEG signals to a rich visual latent space. We evaluate our approach using a broad suite of image reconstruction metrics that have been standardized in the adjacent field of fMRI-to-Image research, and we describe the first EEG-to-Image study to conduct extensive behavioral evaluations of our reconstructions using human raters. Our simple and robust architecture provides significant performance improvements across both research-grade and consumer-grade EEG hardware, and provides a substantial boost in cross-subject decoding alignment. Finally, we provide extensive ablations to determine the architectural choices most responsible for our performance gains in both single and multi-subject cases across multiple benchmark datasets. Collectively our work provides a substantial step towards the development of practical brain-computer interface applications.",https://openreview.net/forum?id=GzayLAuiuy,"Reese Kneeland, Wangshu Jiang, Ugo Bruzadin Nunes, Si Kai Lee, Paul Steven Scotti, Arnaud Delorme, Jonathan Xu",Poster
Mouse-Guided Gaze: Semi-Supervised Learning of Intention-Aware Representations for Reading Detection,"Understanding user intent during magnified reading is critical for accessible interface design. Yet magnification collapses visual context and forces continual viewport dragging, producing fragmented, noisy gaze and obscuring reading intent. We present a semi-supervised framework that learns intention-aware gaze representations by leveraging mouse trajectories as weak supervision. The model is first pretrained to predict mouse velocity from unlabeled gaze, then fine-tuned to classify reading versus scanning. To address magnification-induced distortions, we jointly model raw gaze within the magnified viewport and a compensated view remapped to the original screen, which restores spatial continuity across lines and paragraphs. Across text and webpage datasets, our approach consistently outperforms supervised baselines, with semi-supervised pretraining yielding up to 7.5\% F1 improvement in challenging settings. These findings highlight the value of behavior-driven pretraining for robust, gaze-only interaction, paving the way for adaptive, hands-free accessibility tools.",https://openreview.net/forum?id=U72E2lxXWE,"Seongsil Heo, Roberto Manduchi",Poster
MIRAGE: Robust multi-modal architectures translate fMRI-to-image models from vision to mental imagery,"To be useful for downstream applications, vision decoding models that are trained to reconstruct seen images from human brain activity must be able to generalize to internally generated visual representations, i.e., mental images. In an analysis of the recently released NSD-Imagery dataset, we demonstrated that while some modern vision decoders can perform quite well on mental image reconstruction, some fail, and that state-of-the-art (SOTA) performance on seen image reconstruction is no guarantee of SOTA performance on mental image reconstruction. Motivated by these findings, we developed MIRAGE, a method explicitly designed to train on vision datasets and cross-decode mental images from brain activity. MIRAGE employs a linear backbone and multi-modal text and image features as input to a diffusion model. Feature metrics and human raters establish MIRAGE as SOTA for mental image reconstruction on the NSD-Imagery benchmark. With ablation analysis we show that mental image reconstruction works best when decoders use image features with relatively few dimensions and include guidance from text-based and both high- and low-level image-based features. Our work indicates that--given the right architecture--existing large-scale datasets using external stimuli are viable training data for decoding mental images, and warrant optimism about the future success and utility of mental image reconstruction.",https://openreview.net/forum?id=WvwwWgI2fP,"Reese Kneeland, Cesar Torrico, Tong Chen, Jordyn Antonio Ojeda, Shubh Khanna, Jonathan Xu, Paul Steven Scotti, Thomas Naselaris",Poster
The One Where They Brain-Tune for Social Cognition: Multi-Modal Brain-Tuning on Friends,"Recent studies on audio models show brain-tuning–fine-tuning models to better predict corresponding fMRI activity–improves brain alignment and increases performance on downstream semantic and audio tasks. We extend this approach to a multimodal audio-video model to enhance social cognition, targeting the Superior Temporal Sulcus (STS), a key region for social processing, while subjects watch Friends. We find significant increases in brain alignment to the STS and an adjacent ROI, as well as improvements to a social cognition task related to the training data— sarcasm detection in sitcoms. In summary, our study extends brain-tuning to the multi-modal domain, demonstrating improvements to a downstream task after tuning to a relevant functional region.",https://openreview.net/forum?id=tE7alHyFzn,"Nico Policzer, Cameron Braunstein, Mariya Toneva",Poster
Decoding Predictive Inference in Visual Language Processing via Spatiotemporal Neural Coherence,"Human language processing relies on the brain's capacity for predictive inference. We present a machine learning framework for decoding neural (EEG) responses to dynamic visual language stimuli in Deaf signers. Using coherence between neural signals and optical flow-derived motion features, we construct spatiotemporal representations of predictive neural dynamics. Through entropy-based feature selection, we identify frequency-specific neural signatures that differentiate interpretable linguistic input from linguistically disrupted (time-reversed) stimuli. Our results reveal distributed left-hemispheric and frontal low-frequency coherence as key features in language comprehension, with experience-dependent neural signatures correlating with age. This work demonstrates a novel multimodal approach for probing experience-driven generative models of perception in the brain.",https://openreview.net/forum?id=3lV0sF9EJd,"Sean Borneman, Julia Krebs, Ronnie Wilbur, Evguenia Malaia",Poster
Advancing Brainwave Modelling with a Codebook-Based Foundation Model,"Recent advances in large-scale pre-trained Electroencephalogram (EEG) models have shown great promise, driving progress in Brain-Computer Interfaces (BCIs) and healthcare applications. However, despite their success, many existing pre-trained models have struggled to fully capture the rich information content of neural oscillations, a limitation that fundamentally constrains their performance and generalizability across diverse BCI tasks. This limitation is frequently rooted in suboptimal architectural design choices which constrain their representational capacity. In this work, we introduce LaBraM++, an enhanced Large Brainwave Foundation Model (LBM) that incorporates principled improvements grounded in robust signal processing foundations. LaBraM++ demonstrates substantial gains across a variety of tasks, consistently outperforming its originally-based architecture and achieving competitive results when compared to other open-source LBMs. Its superior performance and training efficiency highlight its potential as a strong foundation for future advancements in LBMs.",https://openreview.net/forum?id=rKR2sH50ZE,"Konstantinos Barmpas, Na Lee, Yannis Panagakis, Dimitrios Adamos, Nikolaos Laskaris, Stefanos Zafeiriou",Poster
HuiduRep: A Self-Supervised Learning Framework for More Robust Neural Representations from Extracellular Recordings,"Extracellular recordings are transient voltage fluctuations in the vicinity of neurons, serving as a fundamental modality in neuroscience for decoding brain activity at single-neuron resolution. Spike sorting, the process of attributing each detected spike to its corresponding neuron, is a pivotal step in brain sensing pipelines. However, it remains challenging under low signal-to-noise ratio (SNR), electrode drift, and cross-session variability. In this paper, we propose HuiduRep, a robust self-supervised representation learning framework that extracts discriminative and generalizable features from extracellular recordings. By integrating contrastive learning with a denoising autoencoder, HuiduRep learns latent representations robust to noise and drift. With HuiduRep, we develop a spike sorting pipeline that clusters spike representations without ground truth labels. Experiments on hybrid and real-world datasets demonstrate that HuiduRep achieves strong robustness. Furthermore, the pipeline outperforms state-of-the-art tools such as KiloSort4 and MountainSort5.",https://openreview.net/forum?id=j2ciqxmRP5,"Zishuo Feng, Feng Cao, Wei Shi, Jicong Zhang",Poster